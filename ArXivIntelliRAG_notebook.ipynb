{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b1fda6",
   "metadata": {},
   "source": [
    "Key Components of ArXivIntelliRAG\n",
    "Real-World Research Data\n",
    "The system retrieves live academic papers directly from the arXiv API, providing users with hands-on experience working with current, high-quality scientific literature.\n",
    "\n",
    "Modern NLP Ecosystem\n",
    "It integrates powerful natural language processing techniques, including semantic embeddings, BERT-based relevance scoring, and large language model summarization, to make dense research content easier to navigate and understand.\n",
    "\n",
    "End-to-End Intelligence Pipeline\n",
    "From document indexing to intelligent retrieval and summarization, the project demonstrates a complete, retrieval-augmented generation (RAG) pipeline in action.\n",
    "\n",
    "Practical and User-Centric\n",
    "This tool is built to support researchers, students, and professionals by helping them quickly identify and comprehend the most relevant academic papers aligned with their queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9511cba-c6e5-4807-869c-ead8467143d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is used to get research paper data from the arXiv website using their API.\n",
    "\n",
    "# Importing necessary libraries:\n",
    "# requests is used to make web requests,\n",
    "# BeautifulSoup helps extract specific parts of the XML response,\n",
    "# pandas is used to store and work with the extracted data in table form.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# This function connects to the arXiv API and downloads papers based on a search query.\n",
    "def scrape_arxiv(query, max_results=50):\n",
    "    # This is the base URL for the arXiv API\n",
    "    base_url = \"http://export.arxiv.org/api/query\"\n",
    "    \n",
    "    # These are the parameters we send to the API, including the search keyword and the number of results we want\n",
    "    params = {\n",
    "        \"search_query\": f\"all:{query}\",\n",
    "        \"start\": 0,\n",
    "        \"max_results\": max_results\n",
    "    }\n",
    "    \n",
    "    # Send a GET request to the API with the parameters\n",
    "    response = requests.get(base_url, params=params)\n",
    "    \n",
    "    # If the response is not successful, show an error\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to fetch data from arXiv API\")\n",
    "\n",
    "    # Use BeautifulSoup to parse the response from XML format into a readable structure\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "    \n",
    "    # Create an empty list to store the data for each paper\n",
    "    papers = []\n",
    "    \n",
    "    # Go through each paper entry in the XML and extract required fields\n",
    "    for entry in soup.find_all(\"entry\"):\n",
    "        # Extract the title and remove extra spaces\n",
    "        title = entry.find(\"title\").text.strip()\n",
    "        \n",
    "        # Extract the abstract and remove line breaks\n",
    "        summary = entry.find(\"summary\").text.strip().replace('\\n', ' ')\n",
    "        \n",
    "        # Extract the arXiv URL, which links to the paper's abstract page\n",
    "        arxiv_url = entry.find(\"id\").text.strip()\n",
    "        \n",
    "        # Convert the arXiv abstract page link to a direct PDF link\n",
    "        pdf_url = arxiv_url.replace(\"abs\", \"pdf\") + \".pdf\"\n",
    "        \n",
    "        # Add the extracted information as a dictionary to the papers list\n",
    "        papers.append({\n",
    "            \"title\": title,\n",
    "            \"abstract\": summary,\n",
    "            \"arxiv_url\": arxiv_url,\n",
    "            \"pdf_url\": pdf_url\n",
    "        })\n",
    "    \n",
    "    # Convert the list of paper dictionaries into a pandas DataFrame and return it\n",
    "    return pd.DataFrame(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77fbcaed-3009-4341-8eaa-eb829bbb9613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved papers to arxiv_papers.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jq/jwsf6hg14r9gd574_gfnfl940000gn/T/ipykernel_55385/53044272.py:31: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  soup = BeautifulSoup(response.content, \"lxml\")\n"
     ]
    }
   ],
   "source": [
    "# This cell saves the scraped research papers to a CSV file.\n",
    "\n",
    "# Define the search keyword you want to look up on arXiv\n",
    "query = \"AI Agents\"\n",
    "\n",
    "# Call the scrape_arxiv function with the query to get the list of papers\n",
    "papers_df = scrape_arxiv(query)\n",
    "\n",
    "# Save the list of papers (as a DataFrame) to a CSV file named 'arxiv_papers.csv'\n",
    "# Setting index to False means it won't save row numbers in the CSV\n",
    "papers_df.to_csv(\"arxiv_papers.csv\", index=False)\n",
    "\n",
    "# Print a message to confirm the file has been saved\n",
    "print(\"Saved papers to arxiv_papers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43347d9f-b2a9-491f-868d-f79a91cc7617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell loads the saved research papers from the CSV file so we can work with them again.\n",
    "\n",
    "# Import pandas library to handle data in table format\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file that was saved earlier into a DataFrame\n",
    "papers_df = pd.read_csv(\"arxiv_papers.csv\")\n",
    "\n",
    "# Get the 'abstract' column from the DataFrame and convert it to a list\n",
    "# This will help us process the text of each paper separately\n",
    "abstracts = papers_df[\"abstract\"].tolist()\n",
    "\n",
    "# Get the 'title' column from the DataFrame and convert it to a list\n",
    "# This allows us to match titles with their abstracts later\n",
    "titles = papers_df[\"title\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23195060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    }
   ],
   "source": [
    "# This cell creates vector embeddings from research paper abstracts and stores them in FAISS for fast semantic search.\n",
    "\n",
    "# Import FAISS to build a fast similarity search index\n",
    "# Import HuggingFaceEmbeddings to convert text (abstracts) into numerical vector representations\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Print a message to indicate the start of the embedding process\n",
    "print(\"Generating embeddings...\")\n",
    "\n",
    "# We are using the \"all-MiniLM-L6-v2\" embedding model from the sentence-transformers library\n",
    "# This model is a good trade-off between performance and speed\n",
    "# It generates high-quality sentence embeddings and is much faster and lighter than larger transformer models\n",
    "\n",
    "# Why use this model:\n",
    "# - It‚Äôs designed specifically for generating embeddings for entire sentences or documents (like abstracts)\n",
    "# - It is widely used for semantic similarity tasks, such as retrieving documents related to a search query\n",
    "# - It works well out of the box for most general-purpose retrieval and clustering tasks\n",
    "\n",
    "# Other model options include:\n",
    "# - \"pritamdeka/S-Biomed-Roberta-snli-multinli-stsb\": Better suited for biomedical or scientific text (slightly heavier)\n",
    "# - \"sentence-transformers/all-mpnet-base-v2\": Higher accuracy but slower than MiniLM\n",
    "# - \"allenai/scibert_scivocab_uncased\": More scientific-domain focused, but not directly compatible with sentence-transformers\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Convert each abstract into a vector and store it in FAISS for similarity-based retrieval\n",
    "# FAISS allows us to later find abstracts that are semantically similar to any given query\n",
    "vectorstore = FAISS.from_texts(abstracts, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ddf8bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell sets up the LangChain pipeline to answer questions or summarize using the retrieved abstracts.\n",
    "\n",
    "# Import RetrievalQA to create a question-answering or summarization chain\n",
    "# Import Ollama to use a local or lightweight language model\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "# Initialize the language model using Ollama\n",
    "# We are using the LLaMA 3.2 model which is capable of understanding and summarizing text\n",
    "# The instruct variant helps the model follow clear instructions like \"summarize this\"\n",
    "llm = Ollama(model=\"llama3.2\")  # You can replace this with another LLM like GPT, Gemini or Deepseek  if needed\n",
    "\n",
    "# Convert the FAISS vector store into a retriever object\n",
    "# This retriever will allow the system to find the most relevant abstracts when given a question\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Combine the retriever and the language model into a RetrievalQA pipeline\n",
    "# This pipeline takes a query, retrieves the best matching documents (abstracts), and uses the LLM to answer or summarize\n",
    "qa_pipeline = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88bd901f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# This cell defines a scoring function to rank the most relevant papers using SciBERT.\n",
    "\n",
    "# Import the tokenizer and model for BERT that is trained on scientific papers (SciBERT)\n",
    "# This helps the model better understand scientific vocabulary and sentence structure\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Import softmax to convert model output into probability scores\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the SciBERT model name. This version is trained specifically on scientific text.\n",
    "bert_model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "\n",
    "# Load the tokenizer, which breaks the input text into tokens that the model can understand\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "# Load the SciBERT model for sequence classification. It returns a relevance score for a pair of texts.\n",
    "bert_model = BertForSequenceClassification.from_pretrained(bert_model_name)\n",
    "\n",
    "# This function ranks a list of documents (abstracts) based on how relevant they are to a search query\n",
    "def rank_papers(query, docs):\n",
    "    # Print message to indicate that ranking is in progress\n",
    "    print(\"Ranking using BERT relevance...\")\n",
    "    \n",
    "    # Create a list to store the relevance scores\n",
    "    scores = []\n",
    "\n",
    "    # Go through each abstract in the list of documents\n",
    "    for text in docs:\n",
    "        # Encode the query and the abstract together as input for the model\n",
    "        # Use truncation and padding to make sure input size fits the model\n",
    "        inputs = bert_tokenizer.encode_plus(query, text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "        # Pass the input into the model to get output logits\n",
    "        logits = bert_model(**inputs).logits\n",
    "\n",
    "        # Convert the logits into probabilities using softmax\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        # Take the probability score for the first class as the relevance score\n",
    "        score = probs[0][0].item()\n",
    "\n",
    "        # Add the score to the list\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Combine scores with their corresponding abstracts and sort them from highest to lowest score\n",
    "    return sorted(zip(scores, docs), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "671fc2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking using BERT relevance...\n"
     ]
    }
   ],
   "source": [
    "# This cell retrieves the most relevant abstracts for a specific query and ranks them using the SciBERT model.\n",
    "\n",
    "# Define the user‚Äôs search query.\n",
    "# This is the topic or question we want to explore in the research papers.\n",
    "query = \"AI Agents Evaluation Metrics\"\n",
    "\n",
    "# Use the retriever (based on FAISS + embeddings) to find documents (abstracts) most similar to the query.\n",
    "# This gives us a list of documents that are likely related to the topic.\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Extract just the text content (abstracts) from the retrieved documents.\n",
    "# These will be passed into the ranking function.\n",
    "retrieved_texts = [doc.page_content for doc in retrieved_docs]\n",
    "\n",
    "# Use the SciBERT-based ranking function to sort the documents by how relevant they are to the query.\n",
    "# This helps us prioritize which papers are most likely useful.\n",
    "ranked_results = rank_papers(query, retrieved_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd8852d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Paper Summaries:\n",
      "\n",
      "üìÑ Paper 1: AI Agents: Evolution, Architecture, and Real-World Applications\n",
      "Link to full paper: http://arxiv.org/pdf/2503.12687v1.pdf\n",
      "Abstract: This paper examines the evolution, architecture, and practical applications of AI agents from their early, rule-based incarnations to modern sophisticated systems that integrate large language models with dedicated modules for perception, planning, and tool use. Emphasizing both theoretical foundations and real-world deployments, the paper reviews key agent paradigms, discusses limitations of current evaluation benchmarks, and proposes a holistic evaluation framework that balances task effectiveness, efficiency, robustness, and safety. Applications across enterprise, personal assistance, and specialized domains are analyzed, with insights into future research directions for more resilient and adaptive AI agent systems.\n",
      "üîç Summary: This paper looks at how Artificial Intelligence (AI) agents have developed over time and how they're used in different areas such as businesses, people's homes, and special industries. It discusses the strengths and weaknesses of current AI agents and proposes a new way to evaluate them that considers factors like effectiveness, efficiency, safety, and more. The paper also explores how AI agents can be improved by combining language models with other capabilities like perception and tool use.\n",
      "\n",
      "üìÑ Paper 2: Generative AI as Economic Agents\n",
      "Link to full paper: http://arxiv.org/pdf/2406.00477v1.pdf\n",
      "Abstract: Traditionally, AI has been modeled within economics as a technology that impacts payoffs by reducing costs or refining information for human agents. Our position is that, in light of recent advances in generative AI, it is increasingly useful to model AI itself as an economic agent. In our framework, each user is augmented with an AI agent and can consult the AI prior to taking actions in a game. The AI agent and the user have potentially different information and preferences over the communication, which can result in equilibria that are qualitatively different than in settings without AI.\n",
      "üîç Summary: I don't know how to summarize this research abstract. It appears to be a long introduction to a study about modeling artificial intelligence as an economic agent, but it doesn't provide enough context or information about the actual study being presented.\n",
      "\n",
      "üìÑ Paper 3: Develop AI Agents for System Engineering in Factorio\n",
      "Link to full paper: http://arxiv.org/pdf/2502.01492v1.pdf\n",
      "Abstract: Continuing advances in frontier model research are paving the way for widespread deployment of AI agents. Meanwhile, global interest in building large, complex systems in software, manufacturing, energy and logistics has never been greater. Although AI driven system engineering holds tremendous promise, the static benchmarks dominating agent evaluations today fail to capture the crucial skills required for implementing dynamic systems, such as managing uncertain trade-offs and ensuring proactive adaptability. This position paper advocates for training and evaluating AI agents' system engineering abilities through automation-oriented sandbox games-particularly Factorio. By directing research efforts in this direction, we can equip AI agents with the specialized reasoning and long-horizon planning necessary to design, maintain, and optimize tomorrow's most demanding engineering projects.\n",
      "üîç Summary: Here's a summary of the abstract in simple terms:\n",
      "\n",
      "Researchers think that artificial intelligence (AI) systems are getting better and better, but they're still not good enough at handling complex problems. They want to improve AI systems so they can be used for big, complicated projects like building cars or managing supply chains. To do this, they need new ways to test and train the AI systems, and they think that playing games like Factorio could help them get better at making decisions under uncertainty.\n"
     ]
    }
   ],
   "source": [
    "# This cell summarizes the top-ranked abstracts using the language model pipeline\n",
    "# and also prints the paper title and the arXiv PDF URL.\n",
    "\n",
    "# Print a heading to indicate that summarization is starting\n",
    "print(\"Top Paper Summaries:\")\n",
    "\n",
    "# This is the instruction we give to the language model\n",
    "# It tells the model to convert the abstract into a simpler, more understandable summary\n",
    "summary_prompt = \"Summarize this research abstract in simple terms:\\n\"\n",
    "\n",
    "# Go through the top 3 ranked abstracts (most relevant to the query)\n",
    "for i, (score, abstract) in enumerate(ranked_results[:3], 1):\n",
    "    \n",
    "    # Find the index of the abstract in the original list so we can get title and URLs\n",
    "    idx = abstracts.index(abstract)\n",
    "    \n",
    "    # Extract title and PDF URL from the original DataFrame using the index\n",
    "    title = papers_df[\"title\"][idx]\n",
    "    pdf_url = papers_df[\"pdf_url\"][idx]\n",
    "    \n",
    "    # Print the paper number\n",
    "    print(f\"\\nüìÑ Paper {i}: {title}\")\n",
    "    \n",
    "    # Print the PDF link for the full paper\n",
    "    print(\"Link to full paper:\", pdf_url)\n",
    "    \n",
    "    # Print the original abstract so we know what is being summarized\n",
    "    print(\"Abstract:\", abstract)\n",
    "\n",
    "    # Use the LangChain QA pipeline to generate a summary using the LLM\n",
    "    # We pass the abstract along with the summarization prompt\n",
    "    summary = qa_pipeline.run(f\"{summary_prompt} {abstract}\")\n",
    "\n",
    "    # Print the generated summary\n",
    "    print(\"üîç Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6fe55a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to top_summarized_papers.md\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Build top_results and save to CSV + Markdown\n",
    "# This step compiles the top 3 ranked papers along with their title, abstract, PDF URL, and LLM-generated summary.\n",
    "# The final results are saved in both CSV (structured) and Markdown (readable) formats.\n",
    "\n",
    "# Create an empty list to hold all the results\n",
    "top_results = []\n",
    "\n",
    "# Loop through the top 3 most relevant abstracts based on the query\n",
    "for i, (score, abstract) in enumerate(ranked_results[:3], 1):\n",
    "    \n",
    "    # Find the index of the abstract in the original list to get related metadata\n",
    "    title_idx = abstracts.index(abstract)\n",
    "\n",
    "    # Extract the paper's title and PDF URL from the original DataFrame\n",
    "    title = papers_df[\"title\"][title_idx]\n",
    "    pdf_url = papers_df[\"pdf_url\"][title_idx]\n",
    "\n",
    "    # Create the summarization prompt and pass the abstract to the LLM\n",
    "    summary_prompt = f\"Summarize this research abstract:\\n{abstract}\"\n",
    "    summary = qa_pipeline.run(summary_prompt)\n",
    "\n",
    "    # Store all relevant information in a dictionary and add it to the list\n",
    "    top_results.append({\n",
    "        \"Title\": title,\n",
    "        \"PDF URL\": pdf_url,\n",
    "        \"Abstract\": abstract,\n",
    "        \"Summary\": summary,\n",
    "        \"Relevance Score\": score\n",
    "    })\n",
    "\n",
    "# Convert the list of dictionaries into a pandas DataFrame\n",
    "top_df = pd.DataFrame(top_results)\n",
    "\n",
    "# Save the same results in Markdown format for human-friendly viewing\n",
    "with open(\"top_summarized_papers.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, row in top_df.iterrows():\n",
    "        f.write(f\"## üìÑ Paper {i+1}: {row['Title']}\\n\")\n",
    "        f.write(f\"**PDF Link:** {row['PDF URL']}\\n\\n\")\n",
    "        f.write(f\"**Relevance Score:** {row['Relevance Score']:.4f}\\n\\n\")\n",
    "        f.write(f\"**Abstract:**\\n{row['Abstract']}\\n\\n\")\n",
    "        f.write(f\"**üîç Summary:**\\n{row['Summary']}\\n\\n\")\n",
    "        f.write(\"---\\n\\n\")\n",
    "\n",
    "# Print confirmation after saving both files\n",
    "print(\"Results saved to top_summarized_papers.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d223d4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
